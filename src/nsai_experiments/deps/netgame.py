# file to implent a simple game we can test alpha zero on at a very basic level


import gymnasium as gym
from gymnasium import error, spaces, utils
from gymnasium.utils import seeding
import numpy as np
import random

from nltk.parse.recursivedescent import RecursiveDescentParser
from nltk.tokenize import wordpunct_tokenize

from .grammargame import GrammarAgent, GrammarEnv



class NetGameEnv(gym.Env):
    metadata = {'render.modes': ['human']}

    def __init__(self, nsites=10):
        """
        Every environment should be derived from gym.Env and at least contain the variables observation_space and action_space 
        specifying the type of possible observations and actions using spaces.Box or spaces.Discrete.

        Example:
        >>> EnvTest = FooEnv()
        >>> EnvTest.observation_space=spaces.Box(low=-1, high=1, shape=(3,4))
        >>> EnvTest.action_space=spaces.Discrete(2)
        """
        self.bitflipmode = True  # "setting" a 1 flips it  back to 0
        self.sparsemode = False  # score is only given at end of (fixed length?) episode

        self.nsites = nsites
        self.maxsteps = 2 * nsites if not self.sparsemode else nsites
        self.observation_space = spaces.MultiBinary([self.nsites]) #, seed=42)
        self.action_space = spaces.Discrete(self.nsites)
        self.reset()

    def step(self, action):
        """
        This method is the primary interface between environment and agent.

        Paramters: 
            action: int
                    the index of the respective action (if action space is discrete)

        Returns:
            output: (array, float, bool, dict)
                    information provided by the environment about its current state:
                    (observation, reward, done, info)
        """
#        print ("prestep state <a, s, s(a)>", action, self.state, self.state[action])
        self.attempted_steps += 1
        done = self.attempted_steps == self.maxsteps
        r = -1 - self.bitflipmode  # why bitflipmode affect this ?
        r = -1 

        if action == -1:  # this is being used as a flag for a bad action (e.g., generated by garbage rule)
            return self.state, r, done, {}

        if self.state[action] == 0:
            r = 1
        if self.bitflipmode:
            self.state[action] = 1 - self.state[action]
        else:
            self.state[action] = 1
        done = done or sum(self.state) == self.nsites

        if self.sparsemode:
            r = sum(self.state) if done else 0
        # if done:
        #     print ("Net Episode done <s, r, t, steps>", self.state, r, done, self.attempted_steps)
        normalizer = 1 # playing around with scale of pi vs value loss
        return self.state, r/normalizer, done, {}

    def reset(self, seed = None):
        """
        This method resets the environment to its initial values.

        Returns:
            observation:    array
                            the initial state of the environment
        """
        nones = 2
        if seed is not None: np.random.seed(seed)
        ones = np.random.choice(range(self.nsites), nones, replace=False)
        self.state = np.zeros(self.nsites)
        self.state[ones] = 1
        self.attempted_steps = 0
        return self.state

    def render(self, mode='human', close=False):
        """
        This methods provides the option to render the environment's behavior to a window 
        which should be readable to the human eye if mode is set to 'human'.
        """
        pass

    def close(self):
        """
        This method provides the user with the option to perform any necessary cleanup.
        """
        pass
    
class NetGrammarAgent(GrammarAgent):
    def apply_rules(self, rules):
        # run the rules ('program') generated by a grammar, return an action in the sub-environment in which the rules are 
        # meant to describe optimal action
        nenv = self.game_env
        proposed = -1
        for r in rules:
            # use last rule, in case there's more than one
            if r[0] == 'bit0' and r[1] == 'set1':
                for i in range(nenv.nsites):
                    if nenv.state[i] == 0:
                        proposed = i  # first empty site
            if r[0] == 'bit1' and r[1] == 'set0':
                for i in range(nenv.nsites):
                    if nenv.state[i] == 1:
                        proposed = i
        return proposed  # bad rule or no empty sites


class GrammarNetEnv(gym.Env):
    def __init__(self, nsites=10):
        self.grammarstr =  """
        S -> S1 | S2 
        S1 ->  C A  
        S2 ->  C A C A  
        C -> 'bit0' | 'bit1' 
        A -> 'set0' | 'set1' 
        # C -> 'bit0' | 'bit1' | 'bit2' | 'bit3'
        # A -> 'set0' | 'set1' | 'set3' | 'set4' 
        """
        self.grammar_env = GrammarEnv(self.grammarstr, desired_state_len=6)
        self.game_env = NetGameEnv(nsites)

        self.observation_space = self.grammar_env.observation_space
        self.action_space = self.grammar_env.action_space
        self.agent = NetGrammarAgent(self.game_env, self.grammar_env)

    def step(self, action):
        state, reward, done, rule = self.grammar_env.decode_and_step(action)
        if done:
            parser = RecursiveDescentParser(self.grammar_env.grammar)
            parsed,  = parser.parse(wordpunct_tokenize(rule['rule']))
            # very much grammar specific code:
            # complete rule has been built; 
            # run episode in network env to evaluate rule
            therule = parsed[0]
            c1 = therule[0][0]
            a1 = therule[1][0]
            parsed_rules = [[c1,a1]]
            if len(therule) == 4:
                c2 = therule[2][0]
                a2 = therule[3][0]
                parsed_rules.append([c2,a2])
            self.game_env.reset()
            subdone = False
            while not subdone:
                subaction = self.agent.apply_rules(parsed_rules)  # bad action/rule -> subaction = -1
                substate, subreward, subdone, _ = self.game_env.step(subaction)
                reward += subreward
#            print ("RULE CREATED  ", rule['rule'], "   ", reward)

        return state, reward, done, rule

    def get_action_mask(self, state = None):
        return self.grammar_env.get_action_mask(state)
    
    def reset(self, seed = None):
        self.grammar_env.reset()
        self.game_env.reset()
        return self.grammar_env.state

def make_netgame(nsites=10):
    env = NetGameEnv(nsites)
    return env

def make_grammarnetgame(nsites = 10):
    env = GrammarNetEnv(nsites)
    return env

if __name__=="__main__":
    # random testing of stuff
    env = NetGameEnv(nsites=10)
    grammar_env = GrammarEnv()
    agent = GrammarAgent(env, grammar_env)
    done = False
    while not done:
#        act = agent.random_action()
        act = agent.truly_random_action()
#        obs, rew, done, info = grammar_env.step(act)
        obs, rew, done, info = grammar_env.step_with_mask(act)



