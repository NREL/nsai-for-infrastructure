import gymnasium as gym
import gymnasium.spaces as spaces

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

from alphazeropp.envs.game import EnvGame

from typing import Hashable


class CumulativeRewardWrapper(gym.Wrapper):
    """Wrapper that changes reward behavior: 0 at every step, total steps at termination."""
    
    def __init__(self, env):
        super().__init__(env)
        self.step_count = 0
        self.max_steps = env.max_steps
        self.nsites = env.nsites  # Store the number of sites for later use
    
    def reset(self, **kwargs):
        self.step_count = 0
        return self.env.reset(**kwargs)
    
    def step(self, action):
        observation, reward, terminated, truncated, info = self.env.step(action)
        self.step_count += 1
        if self.step_count >= self.max_steps:
            truncated = True
        
        # Give 0 reward during the episode, final reward at termination
        if terminated or truncated:
            print(f"AM TERMINATED {terminated} OR TRUNCATED {truncated}")
            reward = self.step_count / self.max_steps
        else:
            reward = 0

        # assert reward == 0.0 or (terminated or truncated)
        # assert reward <= 1.0
            
        return observation, reward, terminated, truncated, info

class BitStringGameGym(gym.Env):
    metadata = {'render.modes': ['human']}

    def __init__(self, nsites=10):
        """
        This environment implements a bitstring game where the agent can flip bits in a string.
        Parameters:
            nsites: int
                    the length of the bitstring
        """
        self.bitflipmode = True  # "setting" a 1 flips it  back to 0
        self.sparsemode = True  # score is only given at end of (fixed length?) episode
        self.nones = 2 # number of bits that are initially set to 1

        self.nsites = nsites
        self.max_steps = 2 * nsites if not self.sparsemode else nsites - self.nones
        self.observation_space = spaces.MultiBinary([self.nsites]) #, seed=42)
        self.action_space = spaces.Discrete(self.nsites)
        self.reset()

    def step(self, action):
        """
        This method is the primary interface between environment and agent.

        Paramters: 
            action: int
                    the index of the respective action (if action space is discrete)

        Returns:
            output: (array, float, bool, dict)
                    information provided by the environment about its current state:
                    (observation, reward, done, info)
        """
        #print ("prestep state <a, s, s(a)>", action, self.state, self.state[action])
        self.step_count += 1
        done = self.step_count >= self.max_steps
        """
        What is the following line doing?
        """
        r = -1 - self.bitflipmode  # why bitflipmode affect this ?
        r = -1 

        if action == -1:  # this is being used as a flag for a bad action (e.g., generated by garbage rule)
            return self.state, r, done, {}

        if self.state[action] == 0:
            r = 1
        if self.bitflipmode:
            self.state[action] = 1 - self.state[action]
        else:
            self.state[action] = 1
        done = done or sum(self.state) == self.nsites

        normalizer = self.nsites # playing around with scale of pi vs value loss
        if self.sparsemode:
            r = sum(self.state)/normalizer if done else 0
        # if done:
        #     print ("Net Episode done <s, r, t, steps>", self.state, r, done, self.step_count)
        return self.state, r, done, done, {}

    def reset(self, seed = None):
        """
        Resets the environment to an initial state and returns an initial observation.
        Parameters:
            seed: Optional[int]
                    seed for the random number generator
        """
        if seed is not None:
            np.random.seed(seed)
            torch.manual_seed(seed)
            torch.use_deterministic_algorithms(True, warn_only=True)
        ones = np.random.choice(range(self.nsites), self.nones, replace=False)
        self.state = np.zeros(self.nsites, dtype=np.float32)
        self.state[ones] = 1
        self.step_count = 0
        return self.state, {}


class BitStringGame(EnvGame):
    def __init__(self, use_cumulative_reward_rescale=True, **kwargs):
        env = BitStringGameGym(**kwargs)
        # if use_cumulative_reward_rescale:
        #     env = CumulativeRewardWrapper(env)
        super().__init__(env)
        self._action_mask = np.ones(env.nsites)  # all actions are always available, otherwise it's cheating.
    
    def get_action_mask(self):
        return self._action_mask
    
    @property
    def hashable_obs(self) -> Hashable:
        "Returns a hashable representation of the current observation `obs`."
        return "".join([str(int(x)) for x in self.obs])  + " " + str(self.env.step_count)# Convert the bitstring to a string of '0's and '1's, which is hashable